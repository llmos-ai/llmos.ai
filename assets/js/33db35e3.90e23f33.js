"use strict";(self.webpackChunkllmos_ai=self.webpackChunkllmos_ai||[]).push([[8795],{4:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=s(4848),t=s(8453);const r={sidebar_position:4,title:"Model Service"},o=void 0,l={id:"user_guide/modelservice",title:"Model Service",description:"The LLMOS platform makes it easy to serve machine learning models using the ModelService resource. This tool provides a simple way to set up and manage model serving, powered by the vLLM engine. You can configure details like model name, Hugging Face settings, resource needs, and more to deploy models efficiently and at scale.",source:"@site/docs/user_guide/modelservice.md",sourceDirName:"user_guide",slug:"/user_guide/modelservice",permalink:"/docs/user_guide/modelservice",draft:!1,unlisted:!1,editUrl:"https://github.com/llmos-ai/llmos.ai/tree/main/docs/docs/user_guide/modelservice.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4,title:"Model Service"},sidebar:"tutorialSidebar",previous:{title:"Notebooks",permalink:"/docs/user_guide/notebooks"},next:{title:"GPU Management",permalink:"/docs/category/gpu-management"}},d={},c=[{value:"Creating a Model Service",id:"creating-a-model-service",level:2},{value:"General Configuration",id:"general-configuration",level:3},{value:"Resource Configuration",id:"resource-configuration",level:3},{value:"Volumes",id:"volumes",level:3},{value:"Node Scheduling",id:"node-scheduling",level:3},{value:"Accessing Model Service APIs",id:"accessing-model-service-apis",level:2},{value:"API Endpoints",id:"api-endpoints",level:3},{value:"API Usage Examples",id:"api-usage-examples",level:3},{value:"cURL Example",id:"curl-example",level:4},{value:"Python Example",id:"python-example",level:4},{value:"Notebooks Interaction",id:"notebooks-interaction",level:4},{value:"Model Service Monitoring",id:"model-service-monitoring",level:2},{value:"Adding a Hugging Face Token",id:"adding-a-hugging-face-token",level:2}];function a(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"LLMOS"})," platform makes it easy to serve machine learning models using the ",(0,i.jsx)(n.code,{children:"ModelService"})," resource. This tool provides a simple way to set up and manage model serving, powered by the ",(0,i.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/",children:"vLLM"})," engine. You can configure details like model name, Hugging Face settings, resource needs, and more to deploy models efficiently and at scale."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"model-service-list",src:s(7151).A+"",width:"3110",height:"720"})}),"\n",(0,i.jsx)(n.h2,{id:"creating-a-model-service",children:"Creating a Model Service"}),"\n",(0,i.jsxs)(n.p,{children:["You can create one or more model services from the ",(0,i.jsx)(n.strong,{children:"LLMOS Management > Model Services"})," page."]}),"\n",(0,i.jsx)(n.h3,{id:"general-configuration",children:"General Configuration"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Name and Namespace"}),": Enter the model service name and namespace."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Source and Name"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Select the model source: ",(0,i.jsx)(n.a,{href:"https://huggingface.co/models",children:"Hugging Face"}),", ",(0,i.jsx)(n.a,{href:"https://modelscope.cn/models",children:"ModelScope"}),", or a ",(0,i.jsx)(n.strong,{children:"Local Path"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["For Hugging Face or ModelScope models, paste the model name from the registry (e.g., ",(0,i.jsx)(n.code,{children:"Qwen/Qwen2.5-0.5B-Instruct"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:["For local path models, specify the volume path (e.g., ",(0,i.jsx)(n.code,{children:"/root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Engine Arguments (Optional)"}),": Add arguments like ",(0,i.jsx)(n.code,{children:"--dtype=half --max-model-len=4096"})," in the ",(0,i.jsx)(n.strong,{children:"Arguments"})," field if needed. ",(0,i.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/usage/engine_args.html",children:"More details"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hugging Face Configurations (Optional)"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Use a ",(0,i.jsx)(n.a,{href:"#adding-a-hugging-face-token",children:"secret credential"})," for models that need authentication."]}),"\n",(0,i.jsxs)(n.li,{children:["Add a custom ",(0,i.jsx)(n.strong,{children:"Hugging Face Mirror URL"})," if using a proxy (e.g., ",(0,i.jsx)(n.code,{children:"https://hf-mirror.com/"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environment Variables (Optional)"}),": Add any extra environment variables as needed. ",(0,i.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/usage/env_vars.html",children:"More details"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"model-service-create-general",src:s(1607).A+"",width:"3112",height:"1734"})}),"\n",(0,i.jsx)(n.h3,{id:"resource-configuration",children:"Resource Configuration"}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["For GPU resource requirements of large language models, see ",(0,i.jsx)(n.a,{href:"https://github.com/ray-project/llm-numbers",children:"LLM numbers"}),"."]})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CPU and Memory"}),": Assign CPU and memory resources for the model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPU Resources"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Choose ",(0,i.jsx)(n.strong,{children:"GPU"})," and ",(0,i.jsx)(n.strong,{children:"Runtime Class"})," (default: ",(0,i.jsx)(n.strong,{children:"nvidia"}),").","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Minimum: ",(0,i.jsx)(n.strong,{children:"1 GPU"})," for the ",(0,i.jsx)(n.code,{children:"vllm-openai"})," image."]}),"\n",(0,i.jsxs)(n.li,{children:["For large models, use ",(0,i.jsx)(n.strong,{children:"tensor parallelism"})," to distribute across multiple GPUs on the same node. For example, with 4 GPUs, it will set the tensor parallel size to 4."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["To share a GPU device, enable ",(0,i.jsx)(n.strong,{children:"vGPU"})," and specify the ",(0,i.jsx)(n.code,{children:"vGPU"})," memory size (in MiB) and ",(0,i.jsx)(n.code,{children:"vGPU Cores"})," (default: 100%)."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"model-service-resources",src:s(1072).A+"",width:"3114",height:"1726"})}),"\n",(0,i.jsx)(n.h3,{id:"volumes",children:"Volumes"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Persistent Volume"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Default: A persistent volume mounted at ",(0,i.jsx)(n.code,{children:"/root/.cache"})," stores downloaded models."]}),"\n",(0,i.jsxs)(n.li,{children:["For shared models across services, replace the default volume with a custom ",(0,i.jsx)(n.strong,{children:"ReadWriteMany"})," persistent volume."]}),"\n",(0,i.jsxs)(n.li,{children:["For ",(0,i.jsx)(n.strong,{children:"local path"})," models:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Add an existing volume with model files to skip downloading."}),"\n",(0,i.jsxs)(n.li,{children:["Remove the default ",(0,i.jsx)(n.code,{children:"model-dir"})," volume if unnecessary."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Shared Memory(dshm)"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Mount an ",(0,i.jsx)(n.code,{children:"emptyDir"})," volume to ",(0,i.jsx)(n.code,{children:"/dev/shm"})," with ",(0,i.jsx)(n.strong,{children:"Medium"})," set to ",(0,i.jsx)(n.strong,{children:"Memory"})," for temporary in-memory storage."]}),"\n",(0,i.jsx)(n.li,{children:"Useful for PyTorch tensor parallel inference, which needs shared memory between processes."}),"\n",(0,i.jsx)(n.li,{children:"If not enabled, the default shared memory size is 64 MiB."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"modelservice-create-volumes",src:s(5782).A+"",width:"3172",height:"1622"})}),"\n",(0,i.jsx)(n.h3,{id:"node-scheduling",children:"Node Scheduling"}),"\n",(0,i.jsx)(n.p,{children:"You can specify node constraints for scheduling your model service using node labels, or leave it as default to run on any available node."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"model-service-node-scheduling",src:s(861).A+"",width:"3110",height:"1098"})}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["For more details of the node scheduling, please refer to the ",(0,i.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity",children:"Kubernetes Node Affinity Documentation"}),"."]})}),"\n",(0,i.jsx)(n.h2,{id:"accessing-model-service-apis",children:"Accessing Model Service APIs"}),"\n",(0,i.jsxs)(n.p,{children:["The Model Service exposes a list of RESTful APIs compatible with ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/introduction",children:"OpenAI's API"})," at the ",(0,i.jsx)(n.code,{children:"/v1"})," path. You can get the model API URL by clicking the ",(0,i.jsx)(n.strong,{children:"Copy"})," button of the selected model."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"modelservice-api-url",src:s(6930).A+"",width:"3110",height:"746"})}),"\n",(0,i.jsx)(n.h3,{id:"api-endpoints",children:"API Endpoints"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Route Path"}),(0,i.jsx)(n.th,{children:"Methods"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/v1/chat/completions"})}),(0,i.jsx)(n.td,{children:"POST"}),(0,i.jsx)(n.td,{children:"Perform chat completions using the model service."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/v1/completions"})}),(0,i.jsx)(n.td,{children:"POST"}),(0,i.jsx)(n.td,{children:"Perform standard completions using the model service."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/v1/embeddings"})}),(0,i.jsx)(n.td,{children:"POST"}),(0,i.jsx)(n.td,{children:"Generate embeddings using the model service."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/v1/models"})}),(0,i.jsx)(n.td,{children:"GET"}),(0,i.jsx)(n.td,{children:"List all available models."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/health"})}),(0,i.jsx)(n.td,{children:"GET"}),(0,i.jsx)(n.td,{children:"Check the health of the model service HTTP server."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/tokenize"})}),(0,i.jsx)(n.td,{children:"POST"}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.a,{href:"https://platform.openai.com/tokenizer",children:"Tokenize"})," text using the running model service."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/detokenize"})}),(0,i.jsx)(n.td,{children:"POST"}),(0,i.jsx)(n.td,{children:"Detokenize tokens using the running model service."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/openapi.json"})}),(0,i.jsx)(n.td,{children:"GET, HEAD"}),(0,i.jsx)(n.td,{children:"Get the OpenAPI JSON specification for the model service."})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"api-usage-examples",children:"API Usage Examples"}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["The LLMOS API token can be obtained from the ",(0,i.jsx)(n.a,{href:"../user_and_auth/api-keys",children:"API Keys"})," page."]})}),"\n",(0,i.jsx)(n.h4,{id:"curl-example",children:"cURL Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export LLMOS_API_KEY=myapikey\nexport API_BASE=192.168.31.100:8443/api/v1/namespaces/default/services/modelservice-qwen2:http/proxy/v1\ncurl -k -X POST \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $LLMOS_API_KEY" \\\n  -d \'{\n    "model": "Qwen/Qwen2.5-0.5B-Instruct",\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are a helpful assistant."\n      },\n      {\n        "role": "user",\n        "content": "Say this is a test"\n      }\n    ],\n    "temperature": 0.9\n  }\' \\\n  $API_BASE/chat/completions\n'})}),"\n",(0,i.jsx)(n.p,{children:"Response Example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "id":"chat-efffa70236bd4edda7e5420349339d45",\n  "object":"chat.completion",\n  "created":1727267645,\n  "model":"Qwen/Qwen2.5-0.5B-Instruct",\n  "choices":[\n    {\n      "index":0,\n      "message":{\n        "role":"assistant",\n        "content":"Yes, it is a test."\n      },\n      "logprobs":null,\n      "finish_reason":"stop"\n    }\n  ],\n  "usage":{\n    "prompt_tokens":24,\n    "total_tokens":32,\n    "completion_tokens":8\n  }\n}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"python-example",children:"Python Example"}),"\n",(0,i.jsx)(n.p,{children:"Since the API is compatible with OpenAI, you can use it as a drop-in replacement for OpenAI-based applications."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nimport httpx\n\n# Set up API key and base URL\nopenai_api_key = "llmos-5frck:xxxxxxxxxg79c9p5"\nopenai_api_base = "https://192.168.31.100:8443/api/v1/namespaces/default/services/modelservice-qwen2:http/proxy/v1"\nclient = OpenAI(\n   api_key=openai_api_key,\n   base_url=openai_api_base,\n   http_client=httpx.Client(verify=False), # Disable SSL verification or use a custom CA bundle.\n)\n\ncompletion = client.chat.completions.create(\n   model="Qwen/Qwen2.5-0.5B-Instruct",\n   messages=[{"role": "user", "content": "How do I output all files in a directory using Python?"}]\n)\nprint(completion.choices[0].message.content)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"notebooks-interaction",children:"Notebooks Interaction"}),"\n",(0,i.jsxs)(n.p,{children:["You can also interact with model services using the ",(0,i.jsx)(n.a,{href:"/docs/user_guide/notebooks",children:"Notebooks"}),", which allows you to explore the model\u2019s capabilities more interactively using HTML, graphs, and more (e.g., using a Jupyter Notebook as below)."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"model-service-notebook",src:s(1378).A+"",width:"2870",height:"1752"})}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsx)(n.p,{children:"Within your LLMOS cluster, you can connect to the model service using its internal DNS name."}),(0,i.jsxs)(n.p,{children:["To get the internal DNS name, click the ",(0,i.jsx)(n.strong,{children:"Copy Internal URL"})," button of the model service."]})]}),"\n",(0,i.jsx)(n.h2,{id:"model-service-monitoring",children:"Model Service Monitoring"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"Model Service"})," includes built-in metrics with ",(0,i.jsx)(n.a,{href:"./monitoring/enable-monitoring",children:"LLMOS Monitoring"})," to track performance and usage."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Click on the model service name in the list to open its details page."}),"\n",(0,i.jsxs)(n.li,{children:["Use the ",(0,i.jsx)(n.strong,{children:"Token Metrics"})," tab to view token-level metrics."]}),"\n",(0,i.jsxs)(n.li,{children:["Use the ",(0,i.jsx)(n.strong,{children:"Metrics"})," tab to see resource usage like CPU, memory, and Disk I/O."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"model-service-metrics",src:s(3597).A+"",width:"3238",height:"1746"})}),"\n",(0,i.jsx)(n.h2,{id:"adding-a-hugging-face-token",children:"Adding a Hugging Face Token"}),"\n",(0,i.jsxs)(n.p,{children:["Some models require authentication to download. If your model needs a token, follow these steps to add a ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"Hugging Face token"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Go to ",(0,i.jsx)(n.strong,{children:"Advanced > Secrets"})," and click ",(0,i.jsx)(n.strong,{children:"Create"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Select the ",(0,i.jsx)(n.strong,{children:"Opaque"})," secret type.",(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.img,{alt:"secret-create-opaque",src:s(5093).A+"",width:"3174",height:"916"})]}),"\n",(0,i.jsxs)(n.li,{children:["Choose the ",(0,i.jsx)(n.strong,{children:"Namespace"})," matching your model service and provide a clear ",(0,i.jsx)(n.strong,{children:"Name"})," (e.g., ",(0,i.jsx)(n.code,{children:"my-hf-token"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:["Set the ",(0,i.jsx)(n.strong,{children:"Key"})," to ",(0,i.jsx)(n.code,{children:"token"})," and paste your Hugging Face token as the ",(0,i.jsx)(n.strong,{children:"Value"}),".",(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.img,{alt:"secret-create-hf-token",src:s(265).A+"",width:"3118",height:"1066"})]}),"\n",(0,i.jsxs)(n.li,{children:["Click ",(0,i.jsx)(n.strong,{children:"Create"})," to save the secret."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Once created, the secret will appear as an option when setting up the model service in the same namespace."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},6930:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-api-url-df7afe6f7b2651d80b3b97d6e8045d6f.png"},1607:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-create-general-da51f8ca85d1c17c3dc543d946a3570d.png"},1072:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-create-resources-a05fc23cc4c1d3756a11d9fdcc0187f4.png"},5782:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-create-volumes-4cebb0451994aa20d53c8927aed643e3.png"},7151:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-list-4dfa2a5067cc0c177fa5df7c7d669bdf.png"},3597:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-metrics-b3ec54381cfbdff7a1827c7f6d7e664a.png"},861:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-node-scheduling-7d730f1a68e0add35b1a80b4466810e8.png"},1378:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-notebook-example-e42be55aac23840dfcf65231ddeddc17.png"},265:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/secret-create-hf-token-086bffab7e79dee78f7105417d22e4ce.png"},5093:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/secret-types-opaque-bb0c117ee345442ad345e26636970762.png"},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var i=s(6540);const t={},r=i.createContext(t);function o(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);