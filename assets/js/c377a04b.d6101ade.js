"use strict";(self.webpackChunkllmos_ai=self.webpackChunkllmos_ai||[]).push([[3361],{8321:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var r=n(4848),t=n(8453);const i={sidebar_position:1,title:"Overview"},o="LLMOS Overview",a={id:"index",title:"Overview",description:"LLMOS is an open-source AI infrastructure management software designed to accelerate AI application development and simplify the management of large language models (LLMs).",source:"@site/docs/index.md",sourceDirName:".",slug:"/",permalink:"/docs/",draft:!1,unlisted:!1,editUrl:"https://github.com/llmos-ai/llmos.ai/tree/main/docs/docs/index.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Overview"},sidebar:"tutorialSidebar",next:{title:"Quickstart",permalink:"/docs/quickstart"}},l={},c=[{value:"LLMOS Architecture",id:"llmos-architecture",level:2},{value:"Key Features",id:"key-features",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Next Step",id:"next-step",level:2}];function d(e){const s={a:"a",admonition:"admonition",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"llmos-overview",children:"LLMOS Overview"})}),"\n",(0,r.jsx)(s.p,{children:"LLMOS is an open-source AI infrastructure management software designed to accelerate AI application development and simplify the management of large language models (LLMs)."}),"\n",(0,r.jsx)(s.p,{children:"It supports running on public clouds and private GPU servers, helping you easily deploy private AI models and scale machine learning workflows while reducing the complexity of development and operations."}),"\n",(0,r.jsx)(s.h2,{id:"llmos-architecture",children:"LLMOS Architecture"}),"\n",(0,r.jsx)(s.p,{children:"The following diagram describes the high-level LLMOS architecture:"}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{alt:"LLMOS Architecture",src:n(2818).A+""})}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Server Node"}),": The Server Node is a cloud-based or on-premises machine that hosts the LLMOS platform along with LLMOS-optimized Kubernetes."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Worker Node"}),": The Worker Node is primarily responsible for executing the user workloads."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"LLMOS-Operator"}),": The LLMOS-Operator manages the lifecycle and system components of the LLMOS platform, including LLMOS API-server, LLMOS controllers, and additional system addons."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"LLMOS-Controller"}),": The LLMOS-Controller is responsible for managing the lifecycle and resources like LLM models, notebooks, machine learning cluster, jobs and so on."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Redis"}),": A key-value store used for storing LLMOS's fault-tolerant configurations and API chats."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Workloads"}),": Workloads are computational tasks that run on the LLMOS infrastructure, utilizing requested resources (e.g., CPU, GPU, memory, and storage volumes)."]}),"\n"]}),"\n",(0,r.jsxs)(s.admonition,{type:"note",children:[(0,r.jsx)(s.mdxAdmonitionTitle,{}),(0,r.jsx)(s.p,{children:"Server nodes also function as worker nodes, but prioritize resources to the system components first."})]}),"\n",(0,r.jsx)(s.h2,{id:"key-features",children:"Key Features"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"./quickstart",children:"Easy Installation"}),":"]})," Works out of the box on both x86_64 and ARM64 architectures for a smooth installation experience."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"./user_guide/gpu_management/enable-gpu-stack",children:"GPU Stack Management"}),":"]})," Offers virtual GPU (vGPU) and multi-accelerator support to enhance GPU resource utilization and operational flexibility."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"./user_guide/ml_clusters",children:"Machine Learning Cluster"}),":"]})," Supports distributed computing with parallel processing capabilities and access to leading AI libraries, improving the performance of machine learning workflows\u2014especially for large-scale models and datasets."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"/docs/user_guide/notebooks",children:"Seamless Notebook Integration"}),":"]})," Integrates with popular notebook environments such as ",(0,r.jsx)(s.a,{href:"https://github.com/jupyterlab/jupyterlab",children:"Jupyter Notebook"}),", ",(0,r.jsx)(s.a,{href:"https://github.com/coder/code-server",children:"VSCode"}),", and ",(0,r.jsx)(s.a,{href:"https://github.com/rstudio/rstudio",children:"RStudio"}),", allowing data scientists and developers to work efficiently in familiar tools without complex setup."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"/docs/user_guide/modelservice",children:"ModelService"})," for LLM Serving:"]})," Easily serve LLMs from ",(0,r.jsx)(s.a,{href:"https://huggingface.co/models",children:"HuggingFace"}),", ",(0,r.jsx)(s.a,{href:"https://modelscope.cn/models",children:"ModelScope"})," or local path with ",(0,r.jsx)(s.strong,{children:"OpenAI-compatible APIs"}),"."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"./user_guide/monitoring/enable-monitoring",children:"Monitoring & Alerts"}),":"]})," Easy to track cluster and GPU metrics in real-time with ready-to-use Grafana dashboards, Prometheus rules and alerts."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"./user_guide/storage/system-storage",children:"Storage Management"}),":"]})," Provides built-in distributed storage with high-performance, fault-tolerant features. Offers robust, scalable block and filesystem storage tailored to the demands of AI and LLM applications."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"./user_and_auth/user",children:"User"})," & ",(0,r.jsx)(s.a,{href:"./user_and_auth/role-template",children:"RBAC Management"}),":"]})," Simplifies user management with role-based access control (RBAC) and role templates, ensuring secure and efficient resource allocation."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Optimized for Edge & Branch Deployments:"})," Supports private deployments with optimized resource usage for running models and workloads in edge and branch networks. It also allows for horizontal scaling to accommodate future business needs."]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"AI Research & Development:"})," Simplifies LLM and AI infrastructure management, enabling researchers to focus on innovation rather than operational complexities."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Enterprise AI Solutions:"})," Streamline the deployment of AI applications with scalable infrastructure, making it easier to manage models, storage, and resources across multiple teams."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Data Science Workflows:"})," With notebook integration and powerful cluster computing, LLMOS is ideal for data scientists looking to run complex experiments at scale."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"AI-Driven Products:"})," From chatbots to automated content generation, LLMOS simplifies the process of deploying LLM-based products that can serve millions of users and scale up horizontally."]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"next-step",children:"Next Step"}),"\n",(0,r.jsxs)(s.p,{children:["For supported architecture and installation requirements, please refer to the doc ",(0,r.jsx)(s.a,{href:"./installation/requirements",children:"here"}),"."]}),"\n",(0,r.jsxs)(s.p,{children:["To get started with LLMOS, please refer to the ",(0,r.jsx)(s.a,{href:"./quickstart",children:"Quick Start"})," guide."]})]})}function h(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},2818:(e,s,n)=>{n.d(s,{A:()=>r});const r=n.p+"assets/images/llmos-arch-c8b1ff51c1c65ccc0c5b543687aa2c28.svg"},8453:(e,s,n)=>{n.d(s,{R:()=>o,x:()=>a});var r=n(6540);const t={},i=r.createContext(t);function o(e){const s=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(i.Provider,{value:s},e.children)}}}]);